---
title: "CUNY Spring 2021 Data 622 HW #2"
author: "Adam Rich"
output:
  html_document: 
    theme: "cerulean"
editor_options:
  chunk_output_type: "console"
---


```{r include=FALSE}
options(tinytex.verbose = TRUE)
knitr::opts_chunk$set(
  collapse = TRUE,
  cache = TRUE)
suppressPackageStartupMessages({
  library(alrtools, quietly = TRUE, warn.conflicts = FALSE)
  library(knitr, quietly = TRUE, warn.conflicts = FALSE)
  library(tidyverse, quietly = TRUE, warn.conflicts = FALSE)
  library(tree, quietly = TRUE, warn.conflicts = FALSE)
  library(tidyverse, quietly = TRUE, warn.conflicts = FALSE)
  library(caret, quietly = TRUE, warn.conflicts = FALSE)
  library(pROC, quietly = TRUE, warn.conflicts = FALSE)
  library(broom, quietly = TRUE, warn.conflicts = FALSE)
  library(faraway, quietly = TRUE, warn.conflicts = FALSE)
  library(palmerpenguins, quietly = TRUE, warn.conflicts = FALSE)
  library(e1071, quietly = TRUE, warn.conflicts = FALSE)
  library(GGally, quietly = TRUE, warn.conflicts = FALSE)
})
```



```{r include=FALSE}
source('appendix.R')
```




## Penguins data

The penguins data has 344 rows and 8 columns.
Read more about the data at the 
[package website](https://allisonhorst.github.io/palmerpenguins/articles/intro.html).


```{r eval=FALSE, echo=TRUE}
penguins <- palmerpenguins::penguins %>% 
  dplyr::select(-year)
# Code for info is in appendix
info(penguins)
```
```{r results='asis', echo=FALSE}
penguins <- palmerpenguins::penguins %>% 
  dplyr::select(-year)
info(penguins) %>% 
  dplyr::select(
    column, type, min, max, zero_count, na_count, unique_values, levels) %>% 
  knitr::kable(digits = c(0, 0, 1, 1, 0, 0, 0, 0))
```


### Distribution of Sex

The distribution of penguins by sex is 50/50 for all species.
The species representation in the sample, however, is not equal.

```{r}
table(penguins[, c('species', 'sex')], useNA = "always")
table(penguins$species, useNA = "always") / nrow(penguins)
```




### Islands

Gentoo are all on Biscoe island,
Chinstrap are all on Dream island, 
and Adelie can be found on all three.
I am interested in whether the characteristics of the
three populations of Adelie are different.

```{r}
table(penguins[, c('species', 'island')], useNA = 'always')
adelie <- penguins %>% filter(species == 'Adelie')
table(adelie[, c('island', 'sex')], useNA = 'always')

adelie %>% 
  mutate(ID = 1:nrow(.)) %>% 
  drop_na() %>% 
  dplyr::select(-species, -sex) %>% 
  pivot_longer(
    cols = -c('ID', 'island'), 
    names_to = 'field', 
    values_to = 'value') %>% 
  ggplot() +
  aes(x = value, fill = island) +
  geom_density(alpha = 0.75) +
  facet_wrap(~field, scales = 'free')
```

There is something interesting with `bill_depth_mm`
on Torgerson island, but nothing appears dramatic
enough for me to want to include island.

Based on this sample of birds, if we know the island a bird comes from,
there are at most two species it can belong to.
In the case of Torgersen island, we know it has to be Adelie.
**However, I want to build models that do not rely on island.**






### Pairs Plot

```{r fig.width=6.5, fig.height=6.5, fig.align='center'}
library(GGally)
penguins %>% 
  drop_na %>% 
  dplyr::select(
    species, 
    bill_length_mm, 
    bill_depth_mm, 
    flipper_length_mm, 
    body_mass_g) %>% 
  GGally::ggpairs(
    progress = FALSE,
    mapping = aes(
      fill = species, color = species),
    columns = setdiff(names(.), 'species'))
```


If you look at the distributions of `bill_depth_mm` and `body_mass_g`
and their scatter plot, something really interesting pops out.
There is a perfect linear separation of Gentoo from the other
two species.  It's as if you could use the ratio of bill depth
to body mass to predict whether an individual is Gentoo.






## Part A: Linear Discriminant Analysis

Based on the work above, I will *not* include `island` or `sex` in the models that follow.
But, the four continuous variables all appear helpful in discriminating between the three groups.

Define training and testing sets for all parts.
There are 11 records with NAs -- I'm going to ignore them since there are so few.


```{r}
set.seed(0)
# TRUE means part of training set = 70% of observations
L <- sample(
  c(TRUE, FALSE), 
  size = nrow(penguins), 
  replace = TRUE, 
  prob = c(0.7, 0.3))

ptrain <- penguins %>% filter(L)  %>% 
  drop_na() %>% dplyr::select(-island, -sex)

ptest  <- penguins %>% filter(!L) %>% 
  drop_na() %>% dplyr::select(-island, -sex)

dim(ptrain)
dim(ptest)
```


Use the `MASS` package to run LDA on the training data.

```{r}
lda1 <- MASS::lda(species ~ ., data = ptrain)
print(lda1)
p1 <- predict(lda1)
table(p1$class, ptrain$species)
p1n <- predict(lda1, newdata = ptest)
table(p1n$class, ptest$species)
```

This model mis-classifies 2 observations each in both the training and test data.
The question I have is whether a more parsimonious model could do just as well.
One thing we noticed in HW #1 was that the logistic model didn't converge
when all four measurements were in the model.  However, LDA doesn't have
this problem.  But, a subset of the features worked just fine in the logistic
model, maybe it will here to.
There are not many possible combinations of features, so I can check all of them.

```{r}
# The four variables I care about
cols <- c(
  "bill_length_mm", 
  "bill_depth_mm", 
  "flipper_length_mm", 
  "body_mass_g")

# Build a "masking" matrix
a <- c(TRUE, FALSE)
b <- matrix(c(
  rep(a, each = 8, length.out = 16),
  rep(a, each = 4, length.out = 16),
  rep(a, each = 2, length.out = 16),
  rep(a, each = 1, length.out = 16)),
  nrow = 16)

# Temp vectors to store output
train_error <- c()
test_error <- c()

# Iterate through all but 16th combo
for (i in 1:15) {
  r <- b[i, ]
  cols_i <- cols[r]
  mod_i <- MASS::lda(
    species ~ .,
    data = ptrain[, c(cols_i, 'species')])
  train_error[i] <- sum(predict(mod_i)$class != ptrain$species)
  test_error[i] <- sum(predict(mod_i, newdata = ptest)$class != ptest$species)
}

# Prepare an exhibit
mod_form <- apply(b[1:15, ], 1, function(r) {
  paste(cols[r], collapse = ' + ')
})

data.frame(
  `Model Formula` = mod_form, 
  `Training Error Rate` = train_error / nrow(ptrain),
  `Test Error Rate` = test_error / nrow(ptest),
  check.names = FALSE) %>% 
  arrange(`Test Error Rate`) %>% 
  knitr::kable(digits = c(0, 3, 3))
```

It appears that the model without `flipper_length_mm` performs just
as well as the saturated model with this test set.
This is probably because `flipper_length_mm` and `body_mass_g` 
have high correlation (see the pairs plot above).






## Part B: Quadratic Discriminant Analysis

The `MASS` package also has a function for QDA.
I can't imagine that QDA will perform better than the 3-feature LDA model above.
See the pairs plot -- there is perfect linear separation of Gentoo and the
other two species using body mass and bill depth.  The addition of
bill length separates Adelie and Chinstrap.


```{r}
qda2 <- MASS::qda(
  species ~ bill_length_mm + bill_depth_mm + body_mass_g, data = ptrain)
print(qda2)
p2 <- predict(qda2)
table(p2$class, ptrain$species)
p2n <- predict(qda2, newdata = ptest)
table(p2n$class, ptest$species)
```

The performance is identical!

```{r}
# Iterate through all but 16th combo
for (i in 1:15) {
  r <- b[i, ]
  cols_i <- cols[r]
  mod_i <- MASS::qda(
    species ~ .,
    data = ptrain[, c(cols_i, 'species')])
  train_error[i] <- sum(predict(mod_i)$class != ptrain$species)
  test_error[i] <- sum(predict(mod_i, newdata = ptest)$class != ptest$species)
}

data.frame(
  `Model Formula` = mod_form, 
  `Training Error Rate` = train_error / nrow(ptrain),
  `Test Error Rate` = test_error / nrow(ptest),
  check.names = FALSE) %>% 
  arrange(`Test Error Rate`) %>% 
  knitr::kable(digits = c(0, 3, 3))
```

Because the QDA performs no better than LDA, we should prefer LDA.




## Part C: Naive Bayes

```{r}
library(e1071)
nb3 <- e1071::naiveBayes(species ~ ., data = ptrain)
print(nb3)
p3 <- predict(nb3, ptrain)
table(p3, ptrain$species)
p3n <- predict(nb3, newdata = ptest)
table(p3n, ptest$species)
```

This doesn't perform as well as the others.
It is because the assumption of independence between the features
is inappropriate with this data -- there is *strong* correlation
between the X's.

```{r}
# Iterate through all but 16th combo
for (i in 1:15) {
  r <- b[i, ]
  cols_i <- cols[r]
  mod_i <- naiveBayes(
    species ~ .,
    data = ptrain[, c(cols_i, 'species')])
  train_error[i] <- sum(predict(mod_i, ptrain) != ptrain$species)
  test_error[i] <- sum(predict(mod_i, ptest) != ptest$species)
}

data.frame(
  `Model Formula` = mod_form, 
  `Training Error Rate` = train_error / nrow(ptrain),
  `Test Error Rate` = test_error / nrow(ptest),
  check.names = FALSE) %>% 
  arrange(`Test Error Rate`) %>% 
  knitr::kable(digits = c(0, 3, 3))
```

In this case, the algorithm suggests we should leave off body mass, 
but the performance of LDA is better.



## Part D

My comments about the three model types are included above.
In summary, LDA works best because of the natural linear separation between
the species.  Naive Bayes is probably not appropriate because
of the strong multicollinearity in the X's.
Just like in the logistic models from HW #1, we don't want to include
both body mass and flipper length because they have very high correlation.
In the case of logisitic regression, including both caused the algorithm
to not converge.  With these model types we do not have that problem,
but parsimony is still preferred.




